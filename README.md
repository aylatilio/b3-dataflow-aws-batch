ğŸ”— RepositÃ³rio: [github.com/aylatilio/b3-dataflow-aws-batch](https://github.com/aylatilio/b3-dataflow-aws-batch.git)

# ğŸ¦ b3-dataflow-aws-batch
Pipeline de dados batch na AWS para ingestÃ£o, transformaÃ§Ã£o e anÃ¡lise do Ã­ndice IBOVESPA, com automaÃ§Ã£o via S3, Lambda, Glue e Athena.

---

## ğŸ‘©â€ğŸ’» Autoria
**Ayla Atilio**  
ğŸ“š PÃ³s-graduaÃ§Ã£o em Machine Learning Engineering â€” FIAP  
ğŸ Python | â˜ï¸ AWS | ğŸ“Š Data Engineering  
ğŸ”— [linkedin.com/in/aylaatilio](https://linkedin.com/in/aylaatilio)  
ğŸ”— [github.com/aylatilio](https://github.com/aylatilio)

---

## ğŸ“˜ VisÃ£o Geral
Este projeto demonstra a implementaÃ§Ã£o de um **pipeline batch de engenharia de dados** na **AWS**, utilizando dados pÃºblicos da **B3 (Bolsa de Valores do Brasil)**.  

O fluxo realiza **ingestÃ£o, transformaÃ§Ã£o, catalogaÃ§Ã£o e anÃ¡lise** de dados histÃ³ricos do **Ã­ndice IBOVESPA**, com automaÃ§Ã£o baseada em eventos no S3 e consultas via Athena.  
Projeto desenvolvido como parte do **Tech Challenge 2 â€” FIAP (Machine Learning Engineering)**.

---

### ğŸš€ Objetivo
Demonstrar um pipeline de **dados escalÃ¡vel, modular e automatizado**, que conecta serviÃ§os da AWS em um fluxo **end-to-end**:

**S3 (armazenamento) â†’ Lambda (orquestraÃ§Ã£o) â†’ Glue (ETL) â†’ Athena (anÃ¡lise)**

---

## ğŸ§± Arquitetura

```mermaid
flowchart TD
    A["ğŸ“ˆ yfinance API (IBOV data)"] --> B(["ğŸª£ S3 Bucket raw/ parquet partitioned"])
    B -->|âš¡ S3 Event Trigger| C(["âš™ï¸ AWS Lambda"])
    C --> D(("ğŸ§© AWS Glue Job ETL Transformation"))
    D --> E(["ğŸ’¾ S3 Bucket refined/ parquet + partitions"])
    E --> F{{"ğŸ“š AWS Glue Crawler & Data Catalog"}}
    F --> G["ğŸ” Amazon Athena SQL Queries & Analytics"]
```

ğŸ“ˆ Fonte de dados: API Yahoo Finance (yfinance)  
ğŸ’¾ Armazenamento: AWS S3 (camadas raw e refined)  
âš™ï¸ OrquestraÃ§Ã£o: AWS Lambda + Glue Job ETL  
ğŸ“š CatÃ¡logo e Consulta: AWS Glue Catalog + Amazon Athena

---

## ğŸ—‚ï¸ Estrutura
```plaintext
b3-dataflow-aws-batch/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/               		 # dados brutos (Parquet)
â”‚   â””â”€â”€ refined/           		 # dados transformados localmente
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract_ibov.py          # extraÃ§Ã£o e ingestÃ£o de dados do IBOV via yfinance
â”‚   â”œâ”€â”€ upload_s3.py             # upload automÃ¡tico para o S3 bucket raw/
â”‚   â”œâ”€â”€ lambda_trigger.py        # funÃ§Ã£o Lambda (dispara o Glue Job)
â”‚   â”œâ”€â”€ glue_job_etl.py          # ETL rodando no AWS Glue
â”‚   â””â”€â”€ glue_job_etl_local.py    # SimulaÃ§Ã£o local do Glue Job
â”‚
â”œâ”€â”€ notebooks/ 
â”‚   â””â”€â”€ athena_queries.ipynb  	 # consultas Athena e anÃ¡lises
â”‚
â”œâ”€â”€ docs/                        # documentaÃ§Ã£o e diagramas
â”‚   â””â”€â”€ diagrams/
â”‚       â”œâ”€â”€ architecture.mmd
â”‚       â””â”€â”€ architecture.png
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€.env.example   
â””â”€â”€ README.md
```
---

## ğŸ“˜ Notebook Overview â€” athena_queries.ipynb

O notebook consolida a etapa final do pipeline analÃ­tico, validando a integraÃ§Ã£o entre as camadas do AWS Data Lake (S3, Glue, Athena) e o consumo analÃ­tico em Python. 

ğŸ§© Bloco 1 â€” Query de preview via boto3
Executa uma consulta simples no serviÃ§o Amazon Athena, mas a partir do cÃ³digo Python (boto3), retornando o QueryExecutionId e validando a comunicaÃ§Ã£o programÃ¡tica entre o Glue Catalog e o Athena. 
A mesma query Ã© usada para exibir uma amostra dos 10 registros mais recentes da camada refined, permitindo validar o schema, as partiÃ§Ãµes e a integridade dos dados transformados pelo AWS Glue Job.

ğŸ’¡ Esse bloco garante que o pipeline consegue acionar o Athena via API. 

âš™ï¸ Bloco 2 â€” Query analÃ­tica (volatilidade e amplitude)
Envia uma consulta ao Athena via awswrangler, retornando um DataFrame pandas enriquecido com features derivadas (amplitude, volatilidade_pct). 
Identifica os dias com maior volatilidade (diferenÃ§a entre high e low) e volume negociado, destacando potenciais eventos de pico no Ã­ndice IBOVESPA em 2025. 
Essa anÃ¡lise Ã© base para estudos exploratÃ³rios e criaÃ§Ã£o de features temporais (rolling mean, z-score) ou integraÃ§Ã£o com modelos de detecÃ§Ã£o de anomalias (Isolation Forest, Autoencoders).

ğŸ’¡ Esse bloco comprova que o dataset refinado estÃ¡ pronto para anÃ¡lises quantitativas.

ğŸ“Š Bloco 3 â€” Query agregada de estatÃ­sticas
Calcula medidas de resumo (mÃ©dia, mÃ¡ximo e mÃ­nimo) sobre os preÃ§os do IBOVESPA na camada refined, filtrando o ano de 2025. 
Serve como checagem de consistÃªncia pÃ³s-ETL, garantindo que os valores numÃ©ricos transformados no Glue Job mantÃªm coerÃªncia com o comportamento esperado do Ã­ndice.

ğŸ’¡ Bloco de verificaÃ§Ã£o â€” assegura que a transformaÃ§Ã£o no Glue nÃ£o alterou escalas ou integridade dos dados.

ğŸ§  Bloco 4 â€” Feature Engineering / Isolation Forest Prep
Realiza cÃ¡lculos de mÃ©dia mÃ³vel, desvio padrÃ£o mÃ³vel e z-score a partir da coluna volatilidade_pct, derivada no bloco anterior. Com esses indicadores, Ã© criada a variÃ¡vel volatilidade_pico, 
que recebe valor 1 quando o z-score ultrapassa 1.5 desvios padrÃ£o acima da mÃ©dia, sinalizando perÃ­odos de alta volatilidade, potenciais outliers do comportamento normal do Ã­ndice.

ğŸ’¡ Essa etapa prepara o dataset para uso em algoritmos de detecÃ§Ã£o de anomalias, como Isolation Forest, Autoencoders ou DBSCAN, permitindo identificar variaÃ§Ãµes extremas e regimes de mercado atÃ­picos. 

Raw â†’ Refined â†’ Glue â†’ Athena â†’ Python â†’ Feature Engineering

Raw: dados ingeridos e armazenados em S3 (parquet bruto)
Glue: ETL transforma e grava camada refined 
Athena: consulta SQL no catÃ¡logo do Glue
Python: acesso programÃ¡tico e anÃ¡lise via boto3/awswrangler
Feature Engineering: cÃ¡lculo de mÃ©tricas e preparaÃ§Ã£o para ML

---

## ğŸ§© Lambda Trigger â€” Glue Orchestration
A funÃ§Ã£o `lambda_trigger_glue_job` monitora o bucket *Raw* (`b3-dataflow-raw/raw/`) e,
ao detectar um novo arquivo `.parquet`, dispara automaticamente o Glue Job `b3-etl-job`.
Essa automaÃ§Ã£o conecta as camadas do pipeline S3 â†’ Lambda â†’ Glue â†’ S3 Refined,
eliminando a necessidade de execuÃ§Ã£o manual.

ğŸ”„ Pipeline de ExecuÃ§Ã£o â€” End-to-End Flow
O projeto implementa um pipeline totalmente automatizado que vai da extraÃ§Ã£o local Ã  anÃ¡lise via Athena, integrando componentes locais e serviÃ§os AWS: 

# S3 (raw) â†’ Lambda Trigger â†’ Glue Job â†’ S3 (refined)

| Etapa                          | Ambiente                 | DescriÃ§Ã£o                                                                                                                                              |
| :----------------------------- | :----------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------- |
| ğŸ **extract_ibov.py**         | Local (VS Code)          | Extrai dados do Ã­ndice IBOVESPA via `yfinance` e gera o arquivo parquet bruto (`data/raw/`).                                                           |
| â˜ï¸ **upload_s3.py**            | Local                    | Faz o upload automÃ¡tico do parquet gerado para o bucket S3 `b3-dataflow-raw/raw/`.                                                                     |
| âš¡ **lambda_trigger.py**        | AWS Lambda               | Monitora o bucket `raw/` e, ao detectar um novo arquivo `.parquet`, dispara o Glue Job automaticamente.                                                |
| ğŸ§© **Glue Job (`b3-etl-job`)** | AWS Glue                 | Executa o processo ETL: transforma, particiona e grava o dataset refinado em `b3-dataflow-refined/`.                                                   |
| ğŸ“Š **athena_queries.ipynb**    | Local (Jupyter Notebook) | Conecta-se ao Athena via `boto3` e `awswrangler` para consultar e analisar os dados refinados (volatilidade, amplitude, estatÃ­sticas agregadas, etc.). |

ğŸ’¡ Esse fluxo garante rastreabilidade completa entre a ingestÃ£o, transformaÃ§Ã£o e anÃ¡lise dos dados na nuvem.

| Componente                | FunÃ§Ã£o                                                    |
| ------------------------- | --------------------------------------------------------- |
| **extract_ibov.py**       | Gera parquet local com sucesso                            |
| **upload_s3.py**          | Upload validado via AWS CLI                               |
| **lambda_trigger.py**     | Dispara Glue Job apÃ³s upload                              |
| **glue_job_etl.py**       | Roda com sucesso no AWS Glue                              |
| **glue_job_etl_local.py** | Gera `ibov_refined.parquet` localmente                    |
| **Bucket Raw**            | ContÃ©m parquet do dia 13/10                               |
| **Bucket Refined**        | ContÃ©m parquet refinado gerado                            |
| **Glue Catalog / Athena** | Tabela `ibov_refined` disponÃ­vel e consultÃ¡vel via Athena |

---

## âš™ï¸ Requisitos e ExecuÃ§Ã£o do Pipeline
```plaintext
ğŸ§© PrÃ©-requisitos
	Python 3.11.9 (recomendado pela estabilidade com awswrangler, pandas e boto3)
	Conta AWS com permissÃµes nos serviÃ§os:
		S3 (leitura e escrita)
		Glue (job, crawler e catalog)
		Lambda (execuÃ§Ã£o e logs)
		Athena (consultas)
	Credenciais AWS configuradas localmente (aws configure)
	Ambiente virtual Python ativo (venv)
	
ğŸš€ Etapas de ExecuÃ§Ã£o
1. ConfiguraÃ§Ã£o do ambiente
	Crie e ative o ambiente virtual:
		python -m venv venv
		source venv/bin/activate      # Linux/Mac
		venv\Scripts\activate         # Windows
		
	Instale as dependÃªncias:
		pip install -r requirements.txt
		
2. IngestÃ£o e extraÃ§Ã£o de dados:
		python src/extract_ibov.py
	
	Baixa as cotaÃ§Ãµes recentes do IBOVESPA a partir da API Yahoo Finance e salva em formato Parquet particionado:
		python src/extract_ibov.py
	Arquivo Parquet salvo localmente em data/raw/year=YYYY/month=MM/day=DD/ibov.parquet
	
3. Upload para o Amazon S3
	Envie os dados gerados para o bucket S3 (camada raw):
		python src/upload_s3.py
	Arquivo Parquet disponÃ­vel em s3://b3-dataflow-raw/raw/year=YYYY/month=MM/day=DD/ibov.parquet
	
4. OrquestraÃ§Ã£o automÃ¡tica via Lambda
	O AWS Lambda Trigger detecta automaticamente o novo arquivo na camada raw e executa o Glue Job ETL configurado no console AWS:
		Transforma os dados (mÃ©dia mÃ³vel, amplitude, volatilidade etc.)
		Salva o resultado em s3://b3-dataflow-refined/refined/...
		Atualiza o Glue Catalog via Crawler
	Logs no CloudWatch (b3-etl-job)
	
5. Consulta e anÃ¡lise
	ApÃ³s a conclusÃ£o do ETL, as consultas podem ser feitas:
		Diretamente no Amazon Athena (console web), ou
		Localmente via notebook notebooks/athena_queries.ipynb
	Execute localmente para validar a leitura do Glue Catalog e gerar anÃ¡lises:
		python -m notebook
	
	Resultados esperados:
		Query de preview (boto3) â†’ valida a conectividade
		Query analÃ­tica (awswrangler) â†’ gera amplitude e volatilidade_pct
		Query agregada â†’ valida consistÃªncia
		Feature engineering (pandas) â†’ prepara para Isolation Forest

6. VisualizaÃ§Ã£o e ValidaÃ§Ã£o
	Verifique no S3 as pastas raw e refined
	Confirme a tabela b3_refined_db no Glue Catalog
	Execute queries no Athena
	Visualize o DataFrame enriquecido e as features criadas no notebook

* Para testes rÃ¡pidos, tambÃ©m Ã© possÃ­vel executar o Glue Job manualmente no console e validar o output no Athena antes de ativar o trigger Lambda.
```

---

ğŸ” Consulta SQL no Amazon Athena

ApÃ³s o crawler atualizar o catÃ¡logo (b3_refined_db.ibov_refined), as consultas podem ser executadas diretamente no Athena.
Exemplo de query analÃ­tica validada durante os testes:
```sql
SELECT
    data,
    preco_abertura,
    preco_fechamento,
    (preco_fechamento - preco_abertura) AS variacao_diaria,
    ROUND(((preco_fechamento - preco_abertura) / preco_abertura) * 100, 2) AS variacao_pct,
    volume_negociado
FROM "b3_refined_db"."ibov_refined"
WHERE year = 2025 AND month = 10
ORDER BY data DESC
LIMIT 10;
```

ğŸ“Š Resultado esperado (amostra):
| data       | preco_abertura | preco_fechamento | variacao_diaria | variacao_pct | volume_negociado |
| ---------- | -------------- | ---------------- | --------------- | ------------ | ---------------- |
| 2025-10-13 | 127.845,00     | 128.430,00       | 585,00          | 0.46 %       | 13 245 200       |
| 2025-10-10 | 128.290,00     | 127.845,00       | âˆ’445,00         | âˆ’0.35 %      | 12 988 500       |
| â€¦          | â€¦              | â€¦                | â€¦               | â€¦            | â€¦                |

ğŸ’¡ Resultado do Pipeline: dados transformados acessÃ­veis no Athena, com colunas renomeadas e cÃ¡lculos derivados gerados pelo Glue Job.

---

## ğŸ§  ConclusÃ£o
O pipeline demonstra a integraÃ§Ã£o completa de serviÃ§os AWS para ingestÃ£o, transformaÃ§Ã£o e anÃ¡lise de dados de mercado.
A arquitetura implementa boas prÃ¡ticas de Data Lake (camadas raw/refined), ETL automatizado via Glue e anÃ¡lise com Athena, garantindo escalabilidade e rastreabilidade.
As features geradas (volatilidade, amplitude, z-score) servem como base para futuras aplicaÃ§Ãµes de Machine Learning, como detecÃ§Ã£o de anomalias com Isolation Forest.

---

## ğŸ”— ReferÃªncias
- [Yahoo Finance API (yfinance)](https://pypi.org/project/yfinance/)
- [AWS Glue Documentation](https://docs.aws.amazon.com/glue/index.html)
- [Amazon Athena Documentation](https://docs.aws.amazon.com/athena/)
- [Mermaid Live Editor](https://mermaid.live)
